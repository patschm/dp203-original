{
	"name": "M3_2_1 Transform with Spark",
	"properties": {
		"folder": {
			"name": "Module 3.2"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "sparky",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "e3b6e97d-9f50-44c5-9e06-46ab3d5d0e29"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/b9fc0ff8-96c9-40f6-b971-1c711883d3a0/resourceGroups/Synapse/providers/Microsoft.Synapse/workspaces/4dn-synapse/bigDataPools/sparky",
				"name": "sparky",
				"type": "Spark",
				"endpoint": "https://4dn-synapse.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparky",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.3",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### Globals"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"global lakepath\r\n",
					"lakepath = 'abfss://files@4dnsynapselake.dfs.core.windows.net/sales_small/'"
				],
				"execution_count": 1
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Transform and Save"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"from pyspark.sql.functions import *\r\n",
					"\r\n",
					"df = spark.read.csv(lakepath + 'csv/customers*.csv', header=True, inferSchema=True)\r\n",
					"\r\n",
					"tran = df.withColumn(\"FullName\", concat_ws(\" \", col(\"FirstName\"), col(\"LastName\")))\r\n",
					"tran = tran.drop(\"FirstName\", \"LastName\")\r\n",
					"#display(tran)\r\n",
					"         \r\n",
					"# Read the data again\r\n",
					"tran.write.mode(\"overwrite\").parquet(lakepath + \"transformed_park\")\r\n",
					"df = spark.read.parquet(lakepath + 'transformed_park/*.parquet')\r\n",
					"display(tran)\r\n",
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Partition Data\r\n",
					"\r\n",
					"PySpark Date and Timestamp Functions are supported on DataFrame and SQL queries and they work similarly to traditional SQL, Date and Time are very important if you are using PySpark for ETL. Most of all these functions accept input as, Date type, Timestamp type, or String. If a String used, it should be in a default format that can be cast to date.\r\n",
					"\r\n",
					"- DateType default format is yyyy-MM-dd \r\n",
					"- TimestampType default format is yyyy-MM-dd HH:mm:ss.SSSS\r\n",
					"- Returns null if the input is a string that can not be cast to Date or Timestamp.\r\n",
					"[Date and Timestamp functions](https://sparkbyexamples.com/pyspark/pyspark-sql-date-and-timestamp-functions/)\r\n",
					""
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"from pyspark.sql.functions import *\r\n",
					"\r\n",
					"df = spark.read.csv(lakepath + 'csv/orders*.csv', header=True, inferSchema=True)\r\n",
					"# DateTime is en-US format (C#) so convert it first\r\n",
					"dated = df.withColumn(\"OrderDateTS\", to_timestamp(col(\"OrderDate\"), \"MM/dd/yyyy HH:mm:ss\"))\r\n",
					"dated = dated.drop(\"OrderDate\")\r\n",
					"dated = dated.withColumn(\"Year\", year(col(\"OrderDateTS\"))).withColumn(\"Month\", month(col(\"OrderDateTS\")))\r\n",
					"\r\n",
					"display(dated)\r\n",
					"\r\n",
					"dated.write.partitionBy(\"Year\", \"Month\").mode(\"overwrite\").parquet(lakepath + \"/partition_data\")\r\n",
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Read partitioned data"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"df = spark.read.parquet(lakepath + \"/partition_data/Year=2022/Month=12\")\r\n",
					"display(df)"
				],
				"execution_count": null
			}
		]
	}
}